{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd.function import Function\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import itertools\n",
    "import random\n",
    "import tqdm\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.fft import fftshift\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import nn, einsum\n",
    "import math\n",
    "import logging\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import classification_report\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data read and reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/hikaku_db/chen-z/Workspace\n"
     ]
    }
   ],
   "source": [
    "cd /project/hikaku_db/chen-z/Workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /project/hikaku_db/data/sleep_SHHS/stages_sig/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bal_Trans_8lay_nodrop_mulchan_eegspec_state\r\n",
      "Bal_Trans_8lay_nodrop_mulchan_eegspec_state_R6\r\n",
      "\u001b[0m\u001b[01;34mBalance_set\u001b[0m/\r\n",
      "PreTest_Trans_8lay_50drop_noW_eegspec_state1\r\n",
      "PreTest_Trans_8lay_50drop_noW_eegspec_state2\r\n",
      "PreTest_Trans_8lay_50drop_noW_eegspec_state3\r\n",
      "PreTest_Trans_8lay_50drop_noW_eegspec_state4\r\n",
      "PreTest_Trans_8lay_nodrop_mulchan_eegspec_state1\r\n",
      "PreTest_Trans_8lay_nodrop_mulchan_eegspec_state2\r\n",
      "PreTest_Trans_8lay_nodrop_mulchan_eegspec_state3\r\n",
      "PreTest_Trans_8lay_nodrop_mulchan_eegspec_state4\r\n",
      "PreTest_Trans_8lay_nodrop_mulchan_eegspec_state5\r\n",
      "PreTest_Trans_8lay_nodrop_mulchan_eegspec_state6\r\n",
      "PreTrain_Trans_8lay_nodrop_mulchan_eegspec_state\r\n",
      "\u001b[01;34mPreTrained_8lay_nodrop\u001b[0m/\r\n",
      "\u001b[01;34mPreTrained_t_Trans_8lay_50drop\u001b[0m/\r\n",
      "PreTrained_t_Trans_8lay_50drop_mulchan_eegspec_stateTrain\r\n",
      "Pre_test_Trans_8lay_50drop_mulchan_eegspec_state1\r\n",
      "Pre_test_Trans_8lay_50drop_mulchan_eegspec_state2\r\n",
      "prelabel.csv\r\n",
      "spec_c3_pre.npy\r\n",
      "spec_c4_pre.npy\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/is/zheng-ch/Transformer/PreTdat\n"
     ]
    }
   ],
   "source": [
    "cd /home/is/zheng-ch/Transformer/PreTdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(786606, 32, 30)\n",
      "(786606, 32, 30)\n",
      "(786606, 2, 32, 30)\n"
     ]
    }
   ],
   "source": [
    "dat1 = np.load('/project/hikaku_db/data/sleep_SHHS/stages_sig/C3_spec_30_np/spec_c3_5007_5793_30.npy')\n",
    "dat2 = np.load('/project/hikaku_db/data/sleep_SHHS/stages_sig/C4_spec_30_np/spec_5007_5793_30.npy')\n",
    "print(dat1.shape)\n",
    "print(dat2.shape)\n",
    "\n",
    "dat = np.concatenate((dat1.reshape(-1,1,32,30), dat2.reshape(-1,1,32,30)), axis=1)\n",
    "print(dat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.read_csv(\"/project/hikaku_db/data/sleep_SHHS/stages_sig/ann_delrecords_5class.csv\", header=None)\n",
    "index = index.astype(int)\n",
    "print(index[5007610:5794216].apply(pd.value_counts))\n",
    "label = index[5007610:5794216].values.tolist()#list\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data,label ,transform = None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "        self.datanum = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        out_data = torch.tensor(self.data[idx]).float()\n",
    "        out_label = torch.tensor(self.label[idx])\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "\n",
    "        return out_data, out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat.shape)\n",
    "# label = index[0:100].values.tolist()#list\n",
    "print(len(label))\n",
    "\n",
    "fixdata = dat[:,:,0:16,:]\n",
    "mean_p1 = np.mean(dat[:,:,16:20,:], axis = 2)\n",
    "mean_p2 = np.mean(dat[:,:,20:24,:], axis = 2)\n",
    "mean_p3 = np.mean(dat[:,:,24:28,:], axis = 2)\n",
    "mean_p4 = np.mean(dat[:,:,28:32,:], axis = 2)\n",
    "num_data = len(dat)\n",
    "\n",
    "ch = 2\n",
    "inputdat = np.concatenate((fixdata,mean_p1.reshape(num_data, ch, 1, 30),mean_p2.reshape(num_data, ch, 1, 30),mean_p3.reshape(num_data, ch, 1, 30),mean_p4.reshape(num_data, ch, 1, 30)),axis=2)\n",
    "print(inputdat.shape)\n",
    "\n",
    "train, test, train_label, test_label = train_test_split(inputdat, np.array(label),test_size = 0.1,stratify = label,random_state = 0)\n",
    "print('train data:',len(train))\n",
    "print('test data:',len(test))\n",
    "\n",
    "train_data_set = Mydatasets(data=train,label=train_label)\n",
    "test_data_set = Mydatasets(data=test,label=test_label)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data_set, batch_size = 64, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data_set, batch_size = 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Transformer model, flatted the EEG channel, allows parallel computation the multi-band from multi-channel. \n",
    "\n",
    "class PreNorm(nn.Module): \n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        inner_dim = dim_head *  heads       ##32(4*8)   \n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)   ##better to dim to dim*3\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        \n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):           ##Register the blocks into whole network\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([])\n",
    "        \n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, time_size, fre_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels , dim_head, dropout = 0.5, emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        assert image_size == 30  ##Time dimensions must equal to 30s\n",
    "        num_patches = 300       #30*5'\n",
    "        patch_dim = time_size * fre_size    #1*4'\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w c) (p1 p2)', p1 = fre_size, p2 = time_size),\n",
    "            nn.Linear(patch_dim, dim),        ##4 to dim\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim)) ##Generate the pos value'\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))                ##Generate the class value'\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        \n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "\n",
    "Transmodel = ViT(\n",
    "    image_size = 30, \n",
    "    time_size = 1, \n",
    "    fre_size = 4, \n",
    "    num_classes = 5, \n",
    "    dim = 32, \n",
    "    depth = 8, \n",
    "    heads = 4, \n",
    "    mlp_dim = 128, \n",
    "    channels = 2,\n",
    "    dim_head = 16\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.AdamW(Transmodel.parameters(), lr=1e-3)\n",
    "\n",
    "#Transmodel.load_state_dict(torch.load('PreTrained_t_Trans_8lay_50drop_mulchan_eegspec_stateTrain'))\n",
    "Transmodel.load_state_dict(torch.load('PreTest_Trans_8lay_50drop_noW_eegspec_state3'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(Transmodel.parameters(), lr=1e-4) ##for test of pretrained\n",
    "EPOCH = 150\n",
    "loss_list = []\n",
    "los_min = 10**10\n",
    "val_loss_lis t= []\n",
    "ac_list = []\n",
    "\n",
    "for epoch in tqdm.tqdm(range(EPOCH)):\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    for _, (inputs, labels) in enumerate(train_dataloader, 0):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = Transmodel(inputs)\n",
    "        loss = criterion(outputs, labels.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count = count+1\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    loss_loss=running_loss/count\n",
    "    loss_list.append(loss_loss)\n",
    "    print('epoch',epoch+1,':finished')\n",
    "    print('train_loss:',loss_loss)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        count=0\n",
    "        running_loss=0.0\n",
    "        pre=list()\n",
    "        lab=list()\n",
    "        \n",
    "        for _, (inputs, labels) in enumerate(test_dataloader, 0):\n",
    "            inputs=inputs.to(DEVICE)\n",
    "            labels=labels.to(DEVICE)\n",
    "            \n",
    "            outputs = Transmodel(inputs)\n",
    "            loss =criterion(outputs, labels.squeeze())\n",
    "            running_loss += loss.item()\n",
    "            count+=1\n",
    "            _, predicted = torch.max(F.softmax(outputs).data, 1)\n",
    "            predicted = predicted.to('cpu')\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            predicted = predicted.tolist()\n",
    "            labels = labels.tolist()\n",
    "            pre.append(predicted)\n",
    "            lab.append(labels)\n",
    "            \n",
    "        loss_loss = running_loss/count\n",
    "        val_loss_list.append(loss_loss)\n",
    "        pre = sum(pre,[])\n",
    "        lab = sum(lab,[])\n",
    "        \n",
    "        print('val_loss:',loss_loss)\n",
    "        cl = classification_report(lab, pre,output_dict=True)\n",
    "        print(cl)\n",
    "        ac_list.append(cl['accuracy'])\n",
    "#         if los_min>loss_loss:\n",
    "#             los_min=loss_loss\n",
    "#             torch.save(Transmodel.state_dict(),'Trans_8lay_state') \n",
    "\n",
    "#        torch.save(Transmodel.state_dict(),'PreTrain_Trans_8lay_nodrop_mulchan_eegspec_state')\n",
    "        torch.save(Transmodel.state_dict(),'PreTest_Trans_8lay_50drop_noW_eegspec_state6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_list[1:])\n",
    "plt.plot(val_loss_list[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_l = pd.DataFrame(loss_list[1:], columns=None)\n",
    "val_loss_l = pd.DataFrame(val_loss_list[1:], columns=None)\n",
    "ac_l = pd.DataFrame(ac_list, columns=None)\n",
    "loss_l.to_csv('/home/is/zheng-ch/Transformer/PreTdat/PreTrained_t_Trans_8lay_50drop/PrenonWTestloss_lst6.csv', index=False)\n",
    "val_loss_l.to_csv('/home/is/zheng-ch/Transformer/PreTdat/PreTrained_t_Trans_8lay_50drop/PrenonWTestval_loss_lst6.csv', index=False)\n",
    "ac_l.to_csv('/home/is/zheng-ch/Transformer/PreTdat/PreTrained_t_Trans_8lay_50drop/PrenonWTestac_lst6.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ac_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss: 0.2842285505784895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1/2 [02:15<02:15, 135.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.9643047360451151, 'recall': 0.9388946225844336, 'f1-score': 0.9514300507724053, 'support': 196693}, '1': {'precision': 0.603467178292449, 'recall': 0.33340904788945674, 'f1-score': 0.4295154722134166, 'support': 26415}, '2': {'precision': 0.890987787309704, 'recall': 0.912200161441949, 'f1-score': 0.901469205125722, 'support': 301037}, '3': {'precision': 0.9026397564933343, 'recall': 0.875841274414967, 'f1-score': 0.8890386132046401, 'support': 97174}, '4': {'precision': 0.8108771465877725, 'recall': 0.9061449521628311, 'f1-score': 0.85586811461468, 'support': 108284}, 'accuracy': 0.8927005508475157, 'macro avg': {'precision': 0.834455320945675, 'recall': 0.7932980116987276, 'f1-score': 0.8054642911861727, 'support': 729603}, 'weighted avg': {'precision': 0.8900059403928737, 'recall': 0.8927005508475157, 'f1-score': 0.8894277139773409, 'support': 729603}}\n",
      "val_loss: 0.2842285505784895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2/2 [04:33<00:00, 136.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.9643047360451151, 'recall': 0.9388946225844336, 'f1-score': 0.9514300507724053, 'support': 196693}, '1': {'precision': 0.603467178292449, 'recall': 0.33340904788945674, 'f1-score': 0.4295154722134166, 'support': 26415}, '2': {'precision': 0.890987787309704, 'recall': 0.912200161441949, 'f1-score': 0.901469205125722, 'support': 301037}, '3': {'precision': 0.9026397564933343, 'recall': 0.875841274414967, 'f1-score': 0.8890386132046401, 'support': 97174}, '4': {'precision': 0.8108771465877725, 'recall': 0.9061449521628311, 'f1-score': 0.85586811461468, 'support': 108284}, 'accuracy': 0.8927005508475157, 'macro avg': {'precision': 0.834455320945675, 'recall': 0.7932980116987276, 'f1-score': 0.8054642911861727, 'support': 729603}, 'weighted avg': {'precision': 0.8900059403928737, 'recall': 0.8927005508475157, 'f1-score': 0.8894277139773409, 'support': 729603}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 2\n",
    "loss_list=[]\n",
    "los_min=10**10\n",
    "val_loss_list=[]\n",
    "ac_list=[]\n",
    "\n",
    "for epoch in tqdm.tqdm(range(EPOCH)):\n",
    "    with torch.no_grad():\n",
    "        count=0\n",
    "        running_loss=0.0\n",
    "        pre=list()\n",
    "        lab=list()\n",
    "        for _, (inputs, labels) in enumerate(test_dataloader, 0):\n",
    "            inputs=inputs.to(DEVICE)\n",
    "            labels=labels.to(DEVICE)\n",
    "            outputs = Transmodel(inputs)\n",
    "            loss =criterion(outputs, labels.squeeze())\n",
    "            running_loss += loss.item()\n",
    "            count+=1\n",
    "            _, predicted = torch.max(F.softmax(outputs).data, 1)\n",
    "            predicted=predicted.to('cpu')\n",
    "            labels=labels.to('cpu')\n",
    "            predicted=predicted.tolist()\n",
    "            labels=labels.tolist()\n",
    "            pre.append(predicted)\n",
    "            lab.append(labels)\n",
    "        loss_loss=running_loss/count\n",
    "        val_loss_list.append(loss_loss)\n",
    "        pre=sum(pre,[])\n",
    "        lab=sum(lab,[])\n",
    "        print('val_loss:',loss_loss)\n",
    "        cl = classification_report(lab, pre,output_dict=True)\n",
    "        print(cl)\n",
    "        ac_list.append(cl['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.index([0])\n",
    "indexes = [i for i,x in enumerate(label) if x == [0]]\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maka_patch_data(dat):\n",
    "    dat=np.array(dat).reshape(-1,2,32,30)\n",
    "    fixdata = dat[:,:,0:16,:]\n",
    "    mean_p1 = np.mean(dat[:,:,16:20,:], axis = 2)\n",
    "    mean_p2 = np.mean(dat[:,:,20:24,:], axis = 2)\n",
    "    mean_p3 = np.mean(dat[:,:,24:28,:], axis = 2)\n",
    "    mean_p4 = np.mean(dat[:,:,28:32,:], axis = 2)\n",
    "    num_data = len(dat)\n",
    "    ch = 2\n",
    "    inputdat = np.concatenate((fixdata,mean_p1.reshape(num_data, ch, 1, 30),mean_p2.reshape(num_data, ch, 1, 30),mean_p3.reshape(num_data, ch, 1, 30),mean_p4.reshape(num_data, ch, 1, 30)),axis=2)\n",
    "    return torch.tensor(inputdat)\n",
    "\n",
    "\n",
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, label , make_patch=False,transform = None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.make_patch=make_patch\n",
    "\n",
    "        self.datanum = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.make_patch:\n",
    "            out_data = torch.tensor(self.data[idx]).float()\n",
    "            out_label = torch.tensor(self.label[idx])\n",
    "        else:\n",
    "            out_data = maka_patch_data(self.data[idx]).float()\n",
    "            out_label = torch.tensor(self.label[idx])\n",
    "        if self.transform:\n",
    "            out_data = self.transform(out_data)\n",
    "\n",
    "        return out_data, out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat.shape)\n",
    "# label = index[0:100].values.tolist()#list\n",
    "print(len(label))\n",
    "fixdata = dat[:,:,0:16,:]\n",
    "mean_p1 = np.mean(dat[:,:,16:20,:], axis = 2)\n",
    "mean_p2 = np.mean(dat[:,:,20:24,:], axis = 2)\n",
    "mean_p3 = np.mean(dat[:,:,24:28,:], axis = 2)\n",
    "mean_p4 = np.mean(dat[:,:,28:32,:], axis = 2)\n",
    "num_data = len(dat)\n",
    "ch = 2\n",
    "inputdat = np.concatenate((fixdata,mean_p1.reshape(num_data, ch, 1, 30),mean_p2.reshape(num_data, ch, 1, 30),mean_p3.reshape(num_data, ch, 1, 30),mean_p4.reshape(num_data, ch, 1, 30)),axis=2)\n",
    "print(inputdat.shape)\n",
    "\n",
    "train, test, train_label, test_label = train_test_split(dat, np.array(label),test_size = 0.1,stratify = label,random_state = 0)\n",
    "print('train data:',len(train))\n",
    "print('test data:',len(test))\n",
    "\n",
    "train_data_set = Mydatasets(data=train,label=train_label)\n",
    "test_data_set = Mydatasets(data=test,label=test_label,make_patch=False)\n",
    "test_data_set_patch = Mydatasets(data=test,label=test_label,make_patch=True)\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data_set, batch_size = 64, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data_set, batch_size = 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##no train test split\n",
    "print(dat.shape)\n",
    "# label = index[0:100].values.tolist()#list\n",
    "print(len(label))\n",
    "fixdata = dat[:,:,0:16,:]\n",
    "mean_p1 = np.mean(dat[:,:,16:20,:], axis = 2)\n",
    "mean_p2 = np.mean(dat[:,:,20:24,:], axis = 2)\n",
    "mean_p3 = np.mean(dat[:,:,24:28,:], axis = 2)\n",
    "mean_p4 = np.mean(dat[:,:,28:32,:], axis = 2)\n",
    "num_data = len(dat)\n",
    "ch = 2\n",
    "\n",
    "inputdat = np.concatenate((fixdata,mean_p1.reshape(num_data, ch, 1, 30),mean_p2.reshape(num_data, ch, 1, 30),mean_p3.reshape(num_data, ch, 1, 30),mean_p4.reshape(num_data, ch, 1, 30)),axis=2)\n",
    "print(inputdat.shape)\n",
    "\n",
    "test_data_set = Mydatasets(data=dat,label=np.array(label),make_patch=False)\n",
    "test_data_set_patch = Mydatasets(data=dat,label=np.array(label),make_patch=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Visualization function \n",
    "\n",
    "class PreNorm(nn.Module): \n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads       ##32(4*8)   \n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)   ##better to dim to dim*3\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out),attn\n",
    "        \n",
    "\n",
    "class Transformer(nn.Module):           ##Register the blocks into whole network\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            vdot,attention=attn(x)\n",
    "            x = vdot + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "    \n",
    "    def get_attention(self,x):\n",
    "        attention_list=[]\n",
    "        for attn, ff in self.layers:\n",
    "            vdot,attention=attn(x)\n",
    "            attention_list.append(attention.cpu())\n",
    "        return attention_list\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, time_size, fre_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels , dim_head, dropout = 0.5, emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        assert image_size == 30  ##Time dimensions must equal to 30s\n",
    "        num_patches = 300       #30*5'\n",
    "        patch_dim = time_size * fre_size    #1*4'\n",
    "        \n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w c) (p1 p2)', p1 = fre_size, p2 = time_size),\n",
    "            nn.Linear(patch_dim, dim),        ##4 to dim\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim)) ##Generate the pos value'\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))                ##Generate the class value'\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return self.mlp_head(x)\n",
    "    \n",
    "    def get_attention(self,img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        attention = self.transformer.get_attention(x)\n",
    "        return attention\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "### Patch visualization ####\n",
    "\n",
    "Transmodel = ViT(\n",
    "    image_size = 30, \n",
    "    time_size = 1, \n",
    "    fre_size = 4, \n",
    "    num_classes = 5, \n",
    "    dim = 32, \n",
    "    depth = 8, \n",
    "    heads = 4, \n",
    "    mlp_dim = 128, \n",
    "    channels = 2,\n",
    "    dim_head = 16\n",
    ")\n",
    "\n",
    "\n",
    "Transmodel .load_state_dict(torch.load('/project/hikaku_db/_koki-od/eeg_vit/PreTest_Trans_8lay_50drop_noW_eegspec_state6'))\n",
    "### Second-wise visualization ####\n",
    "Transmodel_1s= ViT(\n",
    "    image_size = 30, \n",
    "    time_size = 1,  ### 1 second\n",
    "    fre_size = 32, ##### all bands\n",
    "    num_classes = 5, \n",
    "    dim = 32, \n",
    "    depth = 8, \n",
    "    heads = 4, \n",
    "    mlp_dim = 128, \n",
    "    channels = 2,\n",
    "    dim_head = 16\n",
    ")\n",
    "Transmodel_1s .load_state_dict(torch.load('/project/hikaku_db/_koki-od/eeg_vit/PreTest_Trans_8lay_nodrop_noW_eegspec_state6_1second'))\n",
    "#optimizer = torch.optim.AdamW(Transmodel.parameters(), lr=1e-3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def patch_attention(idx,dataset=test_data_set_patch):\n",
    "    Transmodel.eval()\n",
    "    \n",
    "    data=dataset[idx][0]\n",
    "    index = Transmodel(data.reshape(-1,2,20,30))\n",
    "    att_mat = torch.stack(Transmodel.get_attention(data.reshape(-1,2,20,30))).squeeze(1)\n",
    "    #print(att_mat.shape)\n",
    "    att_mat = torch.mean(att_mat, dim=1)\n",
    "    #print(att_mat.shape)\n",
    "    residual_att = torch.eye(att_mat.size(1))\n",
    "    aug_att_mat = att_mat + residual_att\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "    #print(aug_att_mat.shape)\n",
    "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "    joint_attentions[0] = aug_att_mat[0]\n",
    "    \n",
    "    for n in range(1, aug_att_mat.size(0)):\n",
    "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "    \n",
    "    for i, v in enumerate(joint_attentions[-1:]):\n",
    "        # Attention from the output token to the input space.\n",
    "        mask = v[0, 1:].detach().numpy()\n",
    "        #print(mask.shape)\n",
    "        img=np.squeeze(Rearrange('b (h w c) -> b c h w',h = 5, w = 30)(mask.reshape(-1,300)))\n",
    "    \n",
    "    return img,index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec_attention(idx,dataset=test_data_set):\n",
    "    Transmodel_1s.eval()\n",
    "    data=test_data_set[idx][0]\n",
    "    index=Transmodel_1s(data.reshape(-1,2,32,30))\n",
    "    \n",
    "    att_mat = torch.stack(Transmodel_1s.get_attention(data.reshape(-1,2,32,30))).squeeze(1)\n",
    "    #print(att_mat.shape)\n",
    "    att_mat = torch.mean(att_mat, dim=1)\n",
    "    #print(att_mat.shape)\n",
    "    residual_att = torch.eye(att_mat.size(1))\n",
    "    aug_att_mat = att_mat + residual_att\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "    #print(aug_att_mat.shape)\n",
    "    joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "    joint_attentions[0] = aug_att_mat[0]\n",
    "    \n",
    "    for n in range(1, aug_att_mat.size(0)):\n",
    "        joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "    \n",
    "    for i, v in enumerate(joint_attentions[-1:]):\n",
    "        # Attention from the output token to the input space.\n",
    "        mask = v[0, 1:].detach().numpy()\n",
    "        #print(mask.shape)\n",
    "        img=np.squeeze(Rearrange('b (h w c) -> b c h w',h = 1, w = 30)(mask.reshape(-1,60)))\n",
    "    \n",
    "    return img,index,test_data_set[idx][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all(idx):\n",
    "    img_sec,index,label=sec_attention(idx)\n",
    "    print(label)\n",
    "    print(torch.argmax(index).item())\n",
    "    img_patch,index=patch_attention(idx)\n",
    "    print(torch.argmax(index).item())\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 4))\n",
    "    ax1.set_title('Original_1ch')\n",
    "    ax2.set_title('Original_2ch')\n",
    "    original_img1= ax1.imshow(test_data_set[idx][0][0].numpy())\n",
    "    fig.colorbar(original_img1, ax=ax1)\n",
    "    original_img2 = ax2.imshow(test_data_set[idx][0][1].numpy())\n",
    "    fig.colorbar(original_img2, ax=ax2)\n",
    "    plt.show()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 4))\n",
    "    img_sec0=cv2.resize(img_sec[0].reshape(1,30),(30,20))\n",
    "    img_sec1=cv2.resize(img_sec[1].reshape(1,30),(30,20))\n",
    "    ax1.set_title('attention1s_1ch')\n",
    "    ax2.set_title('attention1s_2ch')\n",
    "    sec_img1 = ax1.imshow(img_sec0)\n",
    "    fig.colorbar(sec_img1, ax=ax1)\n",
    "    sec_img2 = ax2.imshow(img_sec1)\n",
    "    fig.colorbar(sec_img2, ax=ax2)\n",
    "    plt.savefig('/home/is/zheng-ch/Transformer/resultvit/N2sameF1attenvit_1s.pdf')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 4))\n",
    "    img_patch0=cv2.resize(img_patch[0].reshape(5,30),(30,20))\n",
    "    img_patch1=cv2.resize(img_patch[1].reshape(5,30),(30,20))\n",
    "    ax1.set_title('attention_patch_1ch')\n",
    "    ax2.set_title('attention_patch_2ch')\n",
    "    patch_img0 = ax1.imshow(img_patch0)\n",
    "    fig.colorbar(patch_img0 , ax=ax1)\n",
    "    #img_patch0.savefig(\"whole_figure.png\")\n",
    "    patch_img1 = ax2.imshow(img_patch1)\n",
    "    fig.colorbar(patch_img1 , ax=ax2)\n",
    "    #cv2.imwrite('ViTattentionc3.pdf',patch_img1)\n",
    "    plt.savefig('/home/is/zheng-ch/Transformer/resultvit/N2sameF1attenvit_patch.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=1033\n",
    "show_all(ls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
